{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# importing pandas for the dataframes\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import math as math\n",
    "\n",
    "# output to notebook\n",
    "%matplotlib inline\n",
    "\n",
    "#importing sys to increase the recursion limit, \n",
    "import sys\n",
    "sys.setrecursionlimit(25000)\n",
    "\n",
    "#importing the higher function reduce for a specific part of the code\n",
    "from functools import reduce\n",
    "\n",
    "\n",
    "#the proces ID to get into bigquery, this information cannot be shared since this is an active startup 'Headingon'\n",
    "projectid = \"vast-art-**********\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "class acceleration:    \n",
    "    def looping(self, vehicle, start_row):\n",
    "        \"\"\"This is the main function for retrieving all the data from bigquery\"\"\"       \n",
    "        total_list = []\n",
    "        total_df= pd.DataFrame(total_list)\n",
    "        partial_list = []\n",
    "        \n",
    "        for i in range(2):\n",
    "            partial_list= self.bigquery(vehicle, start_row)\n",
    "            partial_df = pd.DataFrame(partial_list)\n",
    "            total_df = pd.concat([total_df,partial_df], ignore_index=True)\n",
    "            start_row = 11000 + start_row\n",
    "\n",
    "        return total_df\n",
    "            \n",
    "    \n",
    "    \n",
    "    def bigquery (self,vehicle, start_row):\n",
    "        \"\"\"specif function which targets the bigquery database and reformats it\"\"\"\n",
    "        car_list = []\n",
    "        \n",
    "        end_of_row = start_row + 11000\n",
    "        \n",
    "         #the query to bigquery and readjusting the format\n",
    "        query_df = pd.read_gbq(\"select * from [headingon_params.speed_obd] where vehicle ='%s' order by timestamp asc limit 11000 offset %s \" %(vehicle,start_row), projectid)\n",
    "        transit_df = query_df.drop(['fullname', 'unit'], axis=1)\n",
    "        column_indicators= ['timestamp', 'value', 'vehicle']\n",
    "        main_df = transit_df[column_indicators]\n",
    "        return self.filter_function(vehicle, 0, 11000, main_df, car_list)\n",
    "    \n",
    "    \n",
    "    def filter_function(self,vehicle_name,row, end, dataframe, car_list):\n",
    "        \"\"\"This function filters the data which is pulled by the f(bigquery) and filters out the blocks that will be used\n",
    "            by f(core_function) \"\"\"\n",
    "        #the basecase\n",
    "        if row >= end:\n",
    "            return car_list\n",
    "        else:\n",
    "            #these two if-statements ensures that right rows are identified\n",
    "            if dataframe.iloc[row,1] == 0.0:\n",
    "                row_add = row + 1\n",
    "                if dataframe.iloc[row_add, 1] != 0.0:\n",
    "                    row+=1\n",
    "                    return self.core_function(vehicle_name, row, end, dataframe, car_list)\n",
    "                else:\n",
    "                    row+=1\n",
    "                    return self.filter_function(vehicle_name, row, end, dataframe, car_list)\n",
    "            else:\n",
    "                row+=1\n",
    "                return self.filter_function(vehicle_name, row, end, dataframe, car_list)\n",
    "\n",
    "        \n",
    "    def core_function(self,vehicle_name, row, end, dataframe, car_list):\n",
    "        \"\"\"this function calculates the acceleration over a 3 second window\"\"\"\n",
    "        temp_list = []\n",
    "        timestamp = dataframe.iat[row,0]\n",
    "        for i in range(3):\n",
    "            temp_list.append(dataframe.iat[row,1])\n",
    "            row+=1\n",
    "        average_km = (reduce((lambda x, y: x+y), temp_list) / 3) \n",
    "        car_list.append({'timestamp': timestamp, 'value': average_km, 'vehicle': vehicle_name})\n",
    "        row+=1\n",
    "        return self.filter_function(vehicle_name, row, end, dataframe, car_list)\n",
    "\n",
    "###########################################################################################################################\n",
    "####This part of the class acceleration is my coded version of the Local Outlier Factor ###################################\n",
    "###########################################################################################################################\n",
    "    \n",
    "    \n",
    "    def selecting_k_distance_function(self, dataframe, k, car):\n",
    "        \"\"\"Main function under which all other function for determining the LOF factor is placed\"\"\"\n",
    "        #determines the length of the dataframe\n",
    "        row_dataframe = dataframe['timestamp'].count()\n",
    "        k_value = k + 1\n",
    "        dataframe= dataframe.round(1)\n",
    "        #counter which will be used as x coordinate\n",
    "        dataframe['counter'] = range(len(car1.dataframe_1))\n",
    "        #unique id with x and y coordinates\n",
    "        dataframe['unique_id'] = list(zip(dataframe['counter'], dataframe['value']))\n",
    "        column_layout = ['unique_id']\n",
    "        column_values = []\n",
    "        observation_values = []\n",
    "        #builds the appropriate dataframe size according to the inputs given\n",
    "        for i in range(1, k_value):\n",
    "            dataframe['value_k' + str(i)] = 1000\n",
    "            column_layout.append('value_k' + str(i))\n",
    "            column_values.append('value_k' + str(i))\n",
    "            dataframe['observation_k' + str(i)] = -1\n",
    "            column_layout.append('observation_k' + str(i))\n",
    "            observation_values.append('observation_k' + str(i))\n",
    "        dataframe = dataframe[column_layout]\n",
    "        #calculating the Local reach density\n",
    "        for i in range(1, k_value):\n",
    "            self.all_columns_function(0,dataframe, i)\n",
    "        dataframe['sum for LRD'] =  dataframe[column_values].sum(axis=1)\n",
    "        dataframe['LRD'] = 1/(dataframe['sum for LRD']/k)\n",
    "        \n",
    "        observations =[]\n",
    "        # not sure to be happy or worried about this fact that i can do a for-loop and listcomprehension simultaneously\n",
    "        # reason for this choice: a list comprehension does not change data once '=' is reached, at least that is what my debugger insinuates \n",
    "        for i in dataframe[observation_values]:\n",
    "            observations.append('check_'+ i)\n",
    "            dataframe['check_'+ i] = [dataframe.loc[dataframe.loc[j,i], 'LRD'] for j in range(row_dataframe)]\n",
    "        dataframe['LOF'] = (dataframe[observations].sum(axis=1)/k)/dataframe.loc[:,'LRD']\n",
    "        \n",
    "        \n",
    "        outliers_dataframe = dataframe[dataframe['LOF'] > 1.5]\n",
    "        outliers_counted_rows = len(dataframe[dataframe['LOF'] > 1.5])\n",
    "\n",
    "        print('With the k being %s ' %k)\n",
    "        print('the total rows of OUTLIERS : %s' %outliers_counted_rows)\n",
    "       \n",
    "        \n",
    "       \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    def all_columns_function(self, base_row,dataframe, k):\n",
    "        \"\"\"this function goes composes all the necessary columns\"\"\"\n",
    "        row_dataframe = (dataframe['unique_id'].count() - 1) \n",
    "        if base_row > row_dataframe:\n",
    "            return\n",
    "        row_list_seq = list(range((base_row+1), (row_dataframe + 1)))\n",
    "        row_list_seq.extend(range(base_row))\n",
    "        for i in row_list_seq:\n",
    "            self.min_function(base_row,i ,row_dataframe, dataframe, k)   \n",
    "        base_row +=1\n",
    "        return self.all_columns_function(base_row,dataframe, k)       \n",
    "        \n",
    "            \n",
    "    \n",
    "    def min_function(self,base_row, next_row,row_dataframe,dataframe, k):\n",
    "        \"\"\"This function together with f(euclides_function) determines the lowest euclidean value and places it in the correc\n",
    "            column\"\"\"\n",
    "        if k > 1:\n",
    "            other_column_value = dataframe.loc[base_row,('value_k' + str(k-1))]\n",
    "            current_value = dataframe.loc[base_row, ('value_k' + str(k))]\n",
    "            temp_value = self.euclides_function(base_row,next_row, row_dataframe, dataframe)\n",
    "            if (temp_value < current_value and temp_value > other_column_value):\n",
    "                dataframe.loc[base_row,('value_k' + str(k))] = temp_value\n",
    "                coordinate_of_value = dataframe.iloc[next_row, 0]\n",
    "                coordinate_column = 'observation_k' + str(k)\n",
    "                dataframe.set_value(base_row,coordinate_column,coordinate_of_value[0])\n",
    "            else:\n",
    "                pass\n",
    "        else:\n",
    "            current_value = dataframe.loc[base_row, ('value_k' + str(k))]\n",
    "            temp_value = self.euclides_function(base_row,next_row, row_dataframe, dataframe)\n",
    "            if temp_value < current_value:\n",
    "                dataframe.loc[base_row,('value_k' + str(k))] = temp_value\n",
    "                coordinate_of_value = dataframe.iloc[next_row, 0]\n",
    "                coordinate_column = 'observation_k' + str(k)\n",
    "                dataframe.set_value(base_row,coordinate_column,coordinate_of_value[0])\n",
    "            else:\n",
    "                pass      \n",
    "    \n",
    "    \n",
    "      #the formula to calculate the euclidean distance between two point (as the crows fly)\n",
    "    def euclides_function (self, base_row, next_row, row_dataframe,dataframe):\n",
    "        \"\"\"calculates either the euclidean value or the straight line depending on the circumstance\"\"\"\n",
    "        tuple_base = dataframe.iloc[base_row,0]\n",
    "        tuple_next = dataframe.iloc[next_row,0]  \n",
    "        \n",
    "        if tuple_base[1] == tuple_next[1]:\n",
    "            euclide_distance = abs(tuple_base[0]-tuple_next[0])\n",
    "            return euclide_distance\n",
    "        else:\n",
    "            euclide_distance = math.sqrt(math.pow((tuple_base[0] - tuple_next[0]),2) + \n",
    "                                         math.pow((tuple_base[1] - tuple_next[1]),2))\n",
    "            return euclide_distance\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requesting query... ok.\n",
      "Job ID: job_0YemM68NaJrm2zH6RpGxz7SCGxvv\n",
      "Query running...\n",
      "Query done.\n",
      "Processed: 47.5 MB\n",
      "Standard price: $0.00 USD\n",
      "\n",
      "Retrieving results...\n",
      "Got 11000 rows.\n",
      "\n",
      "Total time taken 2.12 s.\n",
      "Finished at 2017-11-14 11:08:53.\n",
      "Requesting query... ok.\n",
      "Job ID: job_JIxiHTwu19MifgPNAOXlSQ5jIOpz\n",
      "Query running...\n",
      "Query done.\n",
      "Processed: 47.5 MB\n",
      "Standard price: $0.00 USD\n",
      "\n",
      "Retrieving results...\n",
      "Got 11000 rows.\n",
      "\n",
      "Total time taken 2.22 s.\n",
      "Finished at 2017-11-14 11:08:56.\n"
     ]
    }
   ],
   "source": [
    "car1 = acceleration()\n",
    "#car2 = acceleration()\n",
    "#car3 = acceleration()\n",
    "#car4 = acceleration()\n",
    "#car5 = acceleration()\n",
    "#car6 = acceleration()\n",
    "#car7 = acceleration()\n",
    "car1.dataframe_1   = pd.DataFrame(car1.looping('car1', 0))\n",
    "#car2.dataframe_1   = pd.DataFrame(car2.looping('car2', 0))\n",
    "#car3.dataframe_1   = pd.DataFrame(car3.looping('car3', 0))\n",
    "#car4.dataframe_1   = pd.DataFrame(car4.looping('car4', 0))\n",
    "#car5.dataframe_1   = pd.DataFrame(car5.looping('car5', 0))\n",
    "#car6.dataframe_1   = pd.DataFrame(car6.looping('car6', 0))\n",
    "#car7.dataframe_1   = pd.DataFrame(car7.looping('car7', 0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From this histograph you can see that there are some probable outliers. An acceloration of above 40 km/h seems high. To make sure we cover all the basics lets first check the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "car1.datafram_validation = car1.dataframe_1[(car1.dataframe_1['value'] > 35)]\n",
    "car1.datafram_validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(pd.read_gbq(\"select * from [headingon_params.speed_obd]  where timestamp > '2016-10-12 18:18:19' and vehicle = 'car1' and timestamp < '2016-10-12 18:18:26' \", projectid))\n",
    "\n",
    "\n",
    "print(pd.read_gbq(\"select * from [headingon_params.speed_obd]  where timestamp > '2016-10-16 13:31:28' and vehicle = 'car1' and timestamp < '2016-10-16 13:31:35' \", projectid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "in both cases a direct database query shows that the values computed are correct. for the sake of practice we will assume that this could actual be an outlier. What i mean is that if we had more data there could be multiple points this high, even by crossreffencing other data like altitude we could find valid explinations for this high accelaration. However,  We will focus on using a statistical learning algorithm to figure out which datapoints are *'inliers'* and which are *'outliers'*. First let as view the data from an different perspective. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "car1.dataframe_1['counter'] = range(len(car1.dataframe_1))\n",
    "\n",
    "\n",
    "car1.dataframe_1.plot.scatter(x='counter', y='value')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# algorithm selection\n",
    "\n",
    "obvious my experience in ML is still growing thus it seems best to discuss some algorithms that I came accross and discuss which one I think is best suited to this particular situation.\n",
    "\n",
    "So lets start with the actual goal we want to achieve:\n",
    "We have a bunch of observation of average acceloration. There are some that seem to be clear *outliers* but there could be others which are not as clear. So what I am looking for is an algorithm which compute which oberservation are *'inliers'* and which are *'outliers'*. \n",
    "\n",
    "our data set is:\n",
    "* non streaming data (due to data limitation)\n",
    "* cannot be categorised as time series data\n",
    "\n",
    "Therefore, i believe i should look into the two main Density-Based Anomaly detection \n",
    "\n",
    "##  K - nearest neigboor(KNN):\n",
    "This algorithm uses the following basic logic: 'if a observation is closer to the *inliers* then to the *outliers* then it is probably an *inliers* and it will be treated as such'. The trick here is that you tell the algorithm how many of the closes point (k) you use to make this decision. For instance, if **k** is 3 that means that the nearest three points of that particulair observation will be taken and if the majority is *inliers* than that observation is classified as an *inlier* and vice versa. \n",
    "\n",
    "However, this algo supposes that you have at least 2 identifyable groups, in other words you know for sure which are *outliers* and which are not.Though in this example we assume that the two most extreme cases are outliers, i doubt it would be that clear-cut if I could import more data points (which is problematic due to the data limit in notebook). So I want an algorithm that defines the outliers essential by itself.\n",
    "Another important concideration of the algorithm is the number of **k** you should use. there are several *rule-of-thumb* conciderations out there but I have not found a clear cut method. If possible I would like to take this guess element out as well but not sure how realistic this is.\n",
    "\n",
    "## Local Outlier Factor(LOF):\n",
    "The major difference between **knn** as i can see it is that the **lof**, is that it calculates both local and global clusters and their *outliers*. So what does that mean? If we look at the graph than the 2 points in the top are an example of global outliers, since they are far outside of the cluster bellow. But let us assume that we can add more datapoints and several of these are much closer to the *outliers* and thus they begin to form a cluster as well. So there are now 2 clusters. In the **knn** algorithm the smaller newer cluster would certainly be seen as a cluster of *outliers* while in the **lof** algorithm a part of the cluster could be seen as *inliers*, depending on the k-distance (the distance of k nearest neigboor).\n",
    "It is this algorithm that i adopt to get to the question of which outliers I need to remove. then I can calculate the mean of the data and do the same for all 7 cars. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### finding the k_distance:\n",
    "\n",
    "coding objectives:\n",
    "\n",
    "First we need to code something that will search for its k times closes neigboor. Since I could not find any convincing methode to calculate the appropriate k, the function should be able to calculate different k's on command.  \n",
    "We also need to take care that points that are equal height are calculated differently than point that have a diagonal line connecting both points. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### reachability-distance:\n",
    "\n",
    "coding objectives:\n",
    "\n",
    "Now we get into a less clear cut part of the algorithm. Called the *reachanility-distance*. This is the formula:\n",
    "\n",
    "***Max(k_dist(B), dist(A,B))***\n",
    "\n",
    "But according to how I read it this will always result in the same Max. I tried stackoverflow but the only entry I could find had the same question but no understandable answer:\n",
    "\n",
    "https://stats.stackexchange.com/questions/112331/is-my-understanding-of-how-to-calculate-the-reachability-distance-in-local-outli\n",
    "\n",
    "To not get stuck and also not try to attempt to code something that I don't fully grasp I have opted for an alternative route. This means that I will code the  *LRD, Local Reachability Distance* which takes the average of all the k's for that specific point and divides them with 1. what does that mean?\n",
    "\n",
    "* for each observation the program will compute the k nearest neigbors. \n",
    "* create a new column which take all the k nearest neigbors of that observation averages them and divides them by 1, which gives us the Local Reachability Distance\n",
    "* we then calculate the LRD of all the nearest neigbors, this does the following: we have the LRD of the actual observation but we want to compare this LRD with the LRD of the nearest neigbors of the observation, this allows us to answer the a very fundamental question namely: *'Are the nearest neigbors of the observation also the nearest neigbors of the observation '*. If i am the outlier then the people who I concider best friends won't concider me as their best friends because I am the outlier. This element makes it possible for the algorithm to determine the difference between local outliers as opposed to only global outliers. \n",
    "* the last column calculates the LOF (local outlier factor) which has a clear cutoff point of 1,5.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### which k to use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With the k being 3 \n",
      "the total rows of OUTLIERS : 17\n"
     ]
    }
   ],
   "source": [
    "car1.selecting_k_distance_function(car1.dataframe_1, 3, 'car1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With the k being 4 \n",
      "the total rows of OUTLIERS : 16\n"
     ]
    }
   ],
   "source": [
    "car1.selecting_k_distance_function(car1.dataframe_1, 4, 'car1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With the k being 5 \n",
      "the total rows of OUTLIERS : 15\n"
     ]
    }
   ],
   "source": [
    "car1.selecting_k_distance_function(car1.dataframe_1, 5, 'car1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With the k being 6 \n",
      "the total rows of OUTLIERS : 14\n"
     ]
    }
   ],
   "source": [
    "car1.selecting_k_distance_function(car1.dataframe_1, 6, 'car1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With the k being 7 \n",
      "the total rows of OUTLIERS : 13\n"
     ]
    }
   ],
   "source": [
    "car1.selecting_k_distance_function(car1.dataframe_1, 7, 'car1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With the k being 8 \n",
      "the total rows of OUTLIERS : 13\n"
     ]
    }
   ],
   "source": [
    "car1.selecting_k_distance_function(car1.dataframe_1, 8, 'car1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With the k being 9 \n",
      "the total rows of OUTLIERS : 13\n"
     ]
    }
   ],
   "source": [
    "car1.selecting_k_distance_function(car1.dataframe_1, 9, 'car1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## conclusion which k:\n",
    "\n",
    "k=7 seems to be the best given the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# recap and improvements:\n",
    "\n",
    "The LOF algorithm is thus far the most non trivial algo I have encoded. And I found the process very engaging and rewarding.\n",
    "\n",
    "What is less rewarding is the problems I have encountered both with the datalimits (have others encountered this?!).\n",
    "And in order to develop the vizuals I encountered problems, once again, which leads me to the conclusion that i need guidance on this, since just coding like i now do apparently is not the answer."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
